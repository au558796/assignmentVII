---
title: "Assignment 3 - Part 2 - Diagnosing Schizophrenia from Voice"
author: "Riccardo Fusaroli"
date: "October 17, 2017"
output: html_document
---
## Assignment 3 - Diagnosing schizophrenia from voice
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
path<- "C:/Users/Dana/Desktop/METHODS III/assignmentVII"
setwd(path)
library(pacman)
p_load(lmerTest, boot, caret)
df <- read.csv("final_rqa.csv")
df$diagnosis<-as.factor(df$diagnosis)
```

### Question 1
```{r}
##Build a logistic regression to see whether you can diagnose schizophrenia from pitch range only.
m1<-glm(diagnosis ~ range, data = df, family = "binomial")
summary(m1)
exp(m1$coefficients[2]-m1$coefficients[1])#exponentiate (freakin' odds)
inv.logit(m1$coefficients[2]-m1$coefficients[1])#inverse log (probabilities)

##Calculate the different performance measures (accuracy, sensitivity, specificity, PPV, NPV, ROC curve) on a logistic regression using the full dataset. Don't forget the random effects!
m2<-glmer(diagnosis ~ scale(range) + scale(mean) + (1+participant), data = df, family = "binomial")
summary(m2)
df$predictions=predict(m2)
df$dpred[df$predictions>0]="schizophrenia"
df$dpred[df$predictions<0]="control"
df$dpred<-as.factor(df$dpred)
confusionMatrix(data = df$dpred, reference = df$diagnosis, positive = "schizophrenia") 

##Then cross-validate the logistic regression and re-calculate performance on the testing folds. N.B. The cross-validation functions you already have should be tweaked: you need to calculate these new performance measures.
diagnosis = df$dpredictions
dfpredict<-data.frame()
dfpredict$diagnosis = diagnosis

fold_function = function(train_df, test_df,model,x){
train_error = NULL
test_error = NULL
n = 1
folds <- cut(seq(1,nrow(train_df)),breaks=x,labels=FALSE)
for(i in 1:x){
    testIndexes <- which(folds==n,arr.ind=TRUE)
    testData <- train_df[testIndexes, ]
    trainData <- train_df[-testIndexes, ]
    model = model
    train_error[n] = modelr::rmse(model, trainData)
    testData$prediction <- predict(model, testData, allow.new.levels = TRUE)
    test_error[n] = modelr::rmse(model, testData)
    n = n+1
    result = c("folds"=n, "train error"= train_error, "test error"= test_error)
}
    return(result)
}
fold_function(df)
##N.B. the predict() function generates probabilities (the full scale between 0 and 1). A probability > .5 indicates a choice of 1, below a choice of 0.
##N.B. you need to decide whether calculate performance on each single test fold or save all the prediction for test folds in one dataset, so to calculate overall performance.
##N.B. Now you have two levels of structure: subject and study. Should this impact your cross-validation?
```
### Question 2

Which single predictor is the best predictor of diagnosis?

### Question 3
```{r}
##Now it's time to go wild! Use all (voice-related) variables and interactions you can think of. Compare models and select the best performing model you can find.
 
##Remember:
# - Cross-validation or AIC are crucial to build the best model!
# - After choosing the model, train it on all the data you have
# - Save the model: save(modelName, file = "BestModelForever.rda")
# - Create a Markdown that can: a) extract the features from new pitch files (basically your previous markdown), b) load your model (e.g. load("BestModelForever.rda")), and c) predict the diagnosis in the new dataframe.

```
### Question 4: Report the results

METHODS SECTION: how did you analyse the data?

RESULTS SECTION: can you diagnose schizophrenia based on voice? which features are used? Comment on the difference between the different performance measures.

### Bonus question 5
```{r}
##You have some additional bonus data involving speech rate, pauses, etc. Include them in your analysis. Do they improve classification?
```

### Bonus question 6
```{r}
##Logistic regression is only one of many classification algorithms. Try using others and compare performance. Some examples: Discriminant Function, Random Forest, Support Vector Machine, etc. The package caret provides them.
```